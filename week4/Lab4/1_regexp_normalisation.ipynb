{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 4: Regular Expressions and Text Normalisation\n",
    "\n",
    "### Learning Outcomes\n",
    "* Be able to set up a Python and Jupyter notebook environment for text analytics\n",
    "* Understand how to use regular expressions to preprocess text\n",
    "* Know how to carry out text normalisation using the NLTK library\n",
    "\n",
    "### Outline\n",
    "\n",
    "* Getting started: how to set up your environment, Jupyter notebooks introduction\n",
    "* Acquiring raw text data\n",
    "* Regular expressions\n",
    "* Text normalisation \n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to Blackboard (anonymously) or on Teams in the QA channel (with your name), or ask a question in the Wednesday live sessions. \n",
    "\n",
    "As you work through the notebooks, please make a note of any code that is unclear to you.\n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! To understand what's going on inside the methods we use here, make sure to watch the lecture videos for the same week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Setting up your environment\n",
    "\n",
    "We recommend using ```conda``` to create an environment with the correct versions of all the packages you need for these labs. You can install either Anaconda or Miniconda, which will include the ```conda``` program. \n",
    "\n",
    "We provide a .yml file that lists all the packages you will need, and the versions that we have tested the labs with. You can use this file to create your environment as follows.\n",
    "\n",
    "1. Open a terminal. Use the command line to navigate to the directory containing this notebook and the file ```environment.yml```. You can use the command ```cd``` to change directory on the command line.\n",
    "\n",
    "1. Run the conda program by typing ```conda env create -f environment.yml```, then answer any questions that appear on the command line.\n",
    "\n",
    "1. Activate the environment by running the command ```conda activate data_analytics```.\n",
    "\n",
    "1. Relaunch Jupyter: shutdown any running instances, and then type ```jupyter lab``` or ```jupyter notebook``` into your command line, depending on whether you prefer the full Jupyter lab development environment, or the simpler Jupyter notebook.\n",
    "\n",
    "1. Find this notebook and open it up again.\n",
    "\n",
    "You should now be ready to go!\n",
    "\n",
    "The core libraries we will be using in this unit are:\n",
    "\n",
    "- [Datasets](https://huggingface.co/docs/datasets/), produced by HuggingFace, is a hub for lots of interesting text datasets.\n",
    "- [NLTK](https://www.nltk.org), a comprehensive NLP library.\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/user_guide.html), for machine learning and classifier evaluation.\n",
    "- [Gensim](https://radimrehurek.com/gensim/), for topic modelling.\n",
    "\n",
    "The libraries above have good documentation, which is available either online (links above) or via Python itself, e.g. `help(numpy.array)` in the Python interpreter. \n",
    "\n",
    "### Refreshers for Python and Jupyter\n",
    "\n",
    "**Skip this part if you're already familiar with Python and Jupyter notebooks.**\n",
    "\n",
    "This lab assumes you have used Python and Jupyter Notebooks before. \n",
    "\n",
    "For an introduction or refresher on Python, see the [Introduction to Python lab](https://github.com/UoB-COMS21202/lab_sheets_public/tree/master/lab_1) or the University of Bristol [Beginning Python](https://milliams.gitlab.io/beginning_python/) course. If you are a beginner with Python, you might also like to look at Chapter 1 in the NLTK book, which also provides a guide for \"getting started with Python\": https://www.nltk.org/book/ \n",
    "\n",
    "You will need to use Python 3, not Python 2, and Python 3.6 or newer are recommended.\n",
    "\n",
    "The labs will be run on [Jupyter Notebook](http://jupyter.org/), an interactive coding environment embedded in a webpage supporting various programing languages (Python, R, Lua, etc.) through the concept of kernels.  \n",
    "\n",
    "It allows you to enrich your code with complex comments formatted in Markdown and $\\LaTeX$, as well as to place the results of your computation right below your code.\n",
    "\n",
    "Notebooks are organised in cells which can contain either code (in our case, this will be Python code) or text, which can be easily and nicely formatted using the Markdown notation. \n",
    "\n",
    "To edit an already existing cell simply double-click on it. You can use the toolbar to insert new cells, edit and delete them (or use keyboard shortcuts which are very handy to speed up coding). \n",
    "\n",
    "Cells can be run, by hitting `shift+enter` when editing a cell or by clicking on the `Run` button at the top. Running a Markdown cell will simply display the formatted text, while running a code cell will execute the commands executed in it. \n",
    "\n",
    "**Note**: when you run a code cell, all the created variables, implemented functions and imported libraries will be then available to every other code cell. However, it is commonly assumed that cells will be run sequentially in terms of prerequisites. To reset all variables and functions (for debugging) simply click `Kernel > Restart` from the Jupyter menu.\n",
    "\n",
    "#### Markdown \n",
    "\n",
    "Markdown cells allow you to write fancy and simple comments: all of this is written in Markdown - double click on this cell to see the source. An introduction to Markdown syntax can be found [here](https://daringfireball.net/projects/markdown/syntax).\n",
    "\n",
    "As Markdown is translated to HTML upon displaying it also allows you to use pure HTML: more details are available [here](https://daringfireball.net/projects/markdown/syntax#html).\n",
    "\n",
    "Finally, you can also display simple $\\LaTeX$ equations in Markdown thanks to `MathJax` support. For inline equations wrap your equation between `$` symbols; for display mode equations use `$$`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Acquiring Raw Text Data\n",
    "\n",
    "Now, let's get some text data! We'll start with the Reuters dataset, which contains financial newswire articles.  Run the code below to download the data from [HuggingFace's datasets hub](https://huggingface.co/datasets/newsgroup):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 17.9kB [00:00, 4.49MB/s]                   \n",
      "Downloading: 19.9kB [00:00, 4.99MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset reuters21578/ModHayes (download: 7.77 MiB, generated: 19.09 MiB, post-processed: Unknown size, total: 26.87 MiB) to ./data_cache\\reuters21578\\ModHayes\\1.0.0\\98a2ad6a0242627562db83992f9625261854c40a88619322596153a5a16a206c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 8.15M/8.15M [00:14<00:00, 580kB/s] \n",
      "                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reuters21578 downloaded and prepared to ./data_cache\\reuters21578\\ModHayes\\1.0.0\\98a2ad6a0242627562db83992f9625261854c40a88619322596153a5a16a206c. Subsequent calls will reuse this data.\n",
      "Training dataset with 20856 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"reuters21578\", \n",
    "    \"ModHayes\",  # Choose this variant of the dataset\n",
    "    split=\"train\",\n",
    "    cache_dir=\"./data_cache\",  # Save the data here \n",
    ")\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the documents in the dataset like elements in a list. For example, document 3 looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'BankAmerica Corp is not under\\npressure to act quickly on its proposed equity offering and\\nwould do well to delay it because of the stock\\'s recent poor\\nperformance, banking analysts said.\\n    Some analysts said they have recommended BankAmerica delay\\nits up to one-billion-dlr equity offering, which has yet to be\\napproved by the Securities and Exchange Commission.\\n    BankAmerica stock fell this week, along with other banking\\nissues, on the news that Brazil has suspended interest payments\\non a large portion of its foreign debt.\\n    The stock traded around 12, down 1/8, this afternoon,\\nafter falling to 11-1/2 earlier this week on the news.\\n    Banking analysts said that with the immediate threat of the\\nFirst Interstate Bancorp &lt;I> takeover bid gone, BankAmerica is\\nunder no pressure to sell the securities into a market that\\nwill be nervous on bank stocks in the near term.\\n    BankAmerica filed the offer on January 26. It was seen as\\none of the major factors leading the First Interstate\\nwithdrawing its takeover bid on February 9.\\n    A BankAmerica spokesman said SEC approval is taking longer\\nthan expected and market conditions must now be re-evaluated.\\n    \"The circumstances at the time will determine what we do,\"\\nsaid Arthur Miller, BankAmerica\\'s Vice President for Financial\\nCommunications, when asked if BankAmerica would proceed with\\nthe offer immediately after it receives SEC approval.\\n    \"I\\'d put it off as long as they conceivably could,\" said\\nLawrence Cohn, analyst with Merrill Lynch, Pierce, Fenner and\\nSmith.\\n    Cohn said the longer BankAmerica waits, the longer they\\nhave to show the market an improved financial outlook.\\n    Although BankAmerica has yet to specify the types of\\nequities it would offer, most analysts believed a convertible\\npreferred stock would encompass at least part of it.\\n    Such an offering at a depressed stock price would mean a\\nlower conversion price and more dilution to BankAmerica stock\\nholders, noted Daniel Williams, analyst with Sutro Group.\\n    Several analysts said that while they believe the Brazilian\\ndebt problem will continue to hang over the banking industry\\nthrough the quarter, the initial shock reaction is likely to\\nease over the coming weeks.\\n    Nevertheless, BankAmerica, which holds about 2.70 billion\\ndlrs in Brazilian loans, stands to lose 15-20 mln dlrs if the\\ninterest rate is reduced on the debt, and as much as 200 mln\\ndlrs if Brazil pays no interest for a year, said Joseph\\nArsenio, analyst with Birr, Wilson and Co.\\n    He noted, however, that any potential losses would not show\\nup in the current quarter.\\n    With other major banks standing to lose even more than\\nBankAmerica if Brazil fails to service its debt, the analysts\\nsaid they expect the debt will be restructured, similar to way\\nMexico\\'s debt was, minimizing losses to the creditor banks.\\n Reuter\\n',\n",
       " 'text_type': '\"NORM\"',\n",
       " 'topics': [],\n",
       " 'lewis_split': '\"TRAIN\"',\n",
       " 'cgis_split': '\"TRAINING-SET\"',\n",
       " 'old_id': '\"5547\"',\n",
       " 'new_id': '\"4\"',\n",
       " 'places': ['usa', 'brazil'],\n",
       " 'people': [],\n",
       " 'orgs': [],\n",
       " 'exchanges': [],\n",
       " 'date': '26-FEB-1987 15:07:13.72',\n",
       " 'title': 'TALKING POINT/BANKAMERICA &lt;BAC> EQUITY OFFER'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regular Expressions\n",
    "\n",
    "## 2.1 Search\n",
    "\n",
    "Now, let's get to grips with regular expressions. Suppose we want to discover all sentences discussing mergers between companies. A first step would be to find all occurrences of the word 'merger':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n",
      "merger\n"
     ]
    }
   ],
   "source": [
    "import re  # Python regular expressions library\n",
    "\n",
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    matches = re.findall('merger', article['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has given us a list of matches in the variable `all_matches`, which all contain the string 'merger', but not the sentences themselves.\n",
    "This isn't very useful, but we can do better if we define the right regular expression!\n",
    "\n",
    "Regular expressions define a pattern, rather than a specific string, allowing us to generalise our search and retrieve a many different strings that match the pattern.\n",
    "In Python, we differentiate a regular expression from a normal string by putting an 'r' character in front of the string.\n",
    "\n",
    "We can generalise our search by using a _disjunction_, which will match against any one of a set of characters. The disjunction is written inside square brackets. \n",
    "\n",
    "Let's try to retrieve instances of the word \"merger\" followed by any letter. We can write a disjunction that matches any lower case letter as `[a-z]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556\n",
      "merger b\n",
      "merger i\n",
      "merger o\n",
      "merger t\n",
      "merger h\n",
      "merger l\n",
      "merger a\n",
      "merger w\n",
      "merger d\n",
      "merger c\n",
      "merger e\n",
      "merger s\n",
      "merger p\n",
      "merger m\n",
      "merger f\n",
      "merger g\n",
      "merger r\n",
      "merger u\n",
      "merger n\n"
     ]
    }
   ],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    matches = re.findall(r'merger [a-z]', article['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current search only matches a single letter of the word after 'merger'. The length of that following word is variable, so how can we write an expression to match the whole word? \n",
    "\n",
    "Here, we can use a special character, '\\*', which will match against zero or more repetitions of the preceding regular expression. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n",
      "merger specialist \n",
      "merger into \n",
      "merger with \n",
      "merger last \n",
      "merger offer \n",
      "merger transaction \n",
      "merger application \n",
      "merger offers \n",
      "merger provides \n",
      "merger proposal \n",
      "merger that \n",
      "merger while \n",
      "merger as \n",
      "merger takes \n",
      "merger by \n",
      "merger guidelines \n",
      "merger could \n",
      "merger calls \n",
      "merger would \n",
      "merger wave \n",
      "merger recently \n",
      "merger among \n",
      "merger costs \n",
      "merger within \n",
      "merger took \n",
      "merger after \n",
      "merger terms \n",
      "merger or \n",
      "merger completed \n",
      "merger notification \n",
      "merger but \n",
      "merger discussions \n",
      "merger the \n",
      "merger because \n",
      "merger called \n",
      "merger commitment \n",
      "merger should \n",
      "merger agreement \n",
      "merger continues \n",
      "merger announced \n",
      "merger involves \n",
      "merger until \n",
      "merger to \n",
      "merger plans \n",
      "merger was \n",
      "merger in \n",
      "merger on \n",
      "merger negotiations \n",
      "merger expenses \n",
      "merger since \n",
      "merger will \n",
      "merger has \n",
      "merger is \n",
      "merger which \n",
      "merger talks \n",
      "merger each \n",
      "merger involving \n",
      "merger approved \n",
      "merger and \n",
      "merger accord \n",
      "merger acquisition \n",
      "merger appears \n",
      "merger proxy \n",
      "merger are \n",
      "merger shareholders \n",
      "merger at \n",
      "merger we \n",
      "merger this \n",
      "merger plan \n",
      "merger partner \n",
      "merger between \n",
      "merger following \n",
      "merger of \n",
      "merger without \n",
      "merger for \n",
      "merger still \n",
      "merger becomes \n",
      "merger trasaction \n",
      "merger cannot \n"
     ]
    }
   ],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    matches = re.findall(r'merger [a-z]* ', article['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely at that regular expression in the last cell -- why is there a space after '\\*'?\n",
    "\n",
    "Now, let's try to match the preceding word as well. It would be better to match capital letters as well as lower case, which we can do with the disjunction `[a-zA-Z]`. \n",
    "\n",
    "TODO 1: complete the code below to retrieve the words that precede and follow 'merger', including capitalised and lower case words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n",
      "The merger still\n",
      "proposed merger or\n",
      "the merger and\n",
      "latest merger plan\n",
      "this merger has\n",
      " merger between\n",
      "heightened merger or\n",
      "defintive merger agreement\n",
      "share merger acquisition\n",
      "a merger of\n",
      "a merger at\n",
      "into merger negotiations\n",
      "a merger was\n",
      "announced merger of\n",
      "The merger involves\n",
      "preliminary merger proposal\n",
      "Chemical merger is\n",
      "announced merger between\n",
      "found merger partners\n",
      "formal merger agreement\n",
      "the merger but\n",
      " merger into\n",
      "the merger that\n",
      "reflect merger with\n",
      "The merger is\n",
      " merger and\n",
      "proposed merger of\n",
      "satisfactory merger agreement\n",
      "a merger to\n",
      "of merger provides\n",
      "planned merger of\n",
      "a merger partner\n",
      "biggest merger since\n",
      "the merger with\n",
      "proposed merger involving\n",
      "its merger agreement\n",
      "the merger after\n",
      "planned merger with\n",
      "a merger proposal\n",
      "a merger or\n",
      "rail merger case\n",
      "the merger cannot\n",
      "Airlines merger would\n",
      "suspended merger talks\n",
      "a merger trasaction\n",
      "potential merger partners\n",
      "the merger for\n",
      "the merger at\n",
      "end merger while\n",
      "preliminary merger proxy\n",
      "the merger Nolan\n",
      "The merger agreement\n",
      "top merger specialists\n",
      "negotiated merger or\n",
      "proposed merger documents\n",
      "the merger would\n",
      "proposed merger has\n",
      "the merger the\n",
      "Their merger would\n",
      "the merger takes\n",
      "s merger activities\n",
      "a merger attempt\n",
      "subsequent merger will\n",
      "a merger and\n",
      "assisted merger or\n",
      " merger negotiations\n",
      "acceptable merger agreement\n",
      "recent merger that\n",
      "the merger continues\n",
      "own merger guidelines\n",
      "Corp merger proposal\n",
      "preliminary merger agreement\n",
      "resulting merger would\n",
      "the merger plan\n",
      "the merger by\n",
      "corporate merger wave\n",
      "a merger in\n",
      "dlr merger with\n",
      "a merger between\n",
      "cash merger with\n",
      "rejected merger overtures\n",
      "ended merger talks\n",
      "reflect merger in\n",
      "the merger decision\n",
      "unsolicited merger offer\n",
      "the merger until\n",
      "aborted merger agreement\n",
      "their merger last\n",
      "s merger agreement\n",
      "a merger which\n",
      "proposed merger until\n",
      "a merger into\n",
      "its merger and\n",
      "the merger will\n",
      "the merger facility\n",
      "way merger between\n",
      "a merger accord\n",
      "s merger with\n",
      "the merger are\n",
      "to merger with\n",
      "and merger are\n",
      "and merger could\n",
      "That merger was\n",
      "definitive merger and\n",
      "The merger will\n",
      "proposed merger into\n",
      "a merger took\n",
      "proposed merger would\n",
      "through merger of\n",
      "future merger between\n",
      "the merger as\n",
      "share merger agreement\n",
      "its merger into\n",
      "possible merger of\n",
      "conversion merger recently\n",
      "announced merger has\n",
      "definitive merger agreement\n",
      "the merger to\n",
      "a merger as\n",
      "pending merger with\n",
      "the merger has\n",
      "the merger proposal\n",
      "preliminary merger talks\n",
      "a merger could\n",
      "The merger was\n",
      "the merger becomes\n",
      "through merger or\n",
      "for merger of\n",
      "further merger activity\n",
      "Sabena merger could\n",
      " merger this\n",
      "a merger with\n",
      "simultaneous merger into\n",
      "reverse merger last\n",
      "Rainbow merger and\n",
      "announced merger negotiations\n",
      " merger of\n",
      "cash merger proposal\n",
      "The merger of\n",
      "the merger which\n",
      "the merger should\n",
      "A merger at\n",
      "a merger is\n",
      "the merger offer\n",
      "other merger of\n",
      "the merger had\n",
      "the merger called\n",
      "by merger mania\n",
      "of merger with\n",
      "or merger of\n",
      "the merger each\n",
      "revive merger plans\n",
      "the merger into\n",
      "The merger following\n",
      "seeking merger partners\n",
      "announced merger with\n",
      "subsequent merger with\n",
      "the merger agreement\n",
      "the merger on\n",
      "extended merger agreement\n",
      "proposed merger between\n",
      "nonrecurring merger expenses\n",
      "proposed merger at\n",
      "to merger of\n",
      "rampant merger mania\n",
      "announced merger agreement\n",
      "from merger of\n",
      "its merger last\n",
      "dlr merger of\n",
      "proposed merger and\n",
      "the merger announced\n",
      "a merger plan\n",
      "proposed merger was\n",
      "The merger took\n",
      "the merger accord\n",
      "Baker merger on\n",
      "possible merger with\n",
      "negotiated merger with\n",
      "their merger will\n",
      "proposed merger appears\n",
      "of merger analysis\n",
      "their merger completed\n",
      "the merger took\n",
      "the merger between\n",
      "way merger would\n",
      "on merger speculation\n",
      "a merger among\n",
      "the merger without\n",
      "unsolicited merger proposal\n",
      "cash merger of\n",
      "and merger and\n",
      "TBS merger agreement\n",
      "their merger agreement\n",
      "the merger of\n",
      "The merger plan\n",
      "the merger was\n",
      "its merger commitment\n",
      "proposed merger is\n",
      "the merger we\n",
      "any merger or\n",
      "Rainbow merger involves\n",
      "proposed merger that\n",
      "a merger agreement\n",
      "pending merger of\n",
      "The merger would\n",
      "The merger calls\n",
      "its merger with\n",
      "a merger would\n",
      "pursue merger talks\n",
      "proposed merger with\n",
      "the merger shareholders\n",
      "Progressive merger because\n",
      "of merger Kaiser\n",
      "the merger because\n",
      "announced merger into\n",
      "the merger within\n",
      "step merger agreement\n",
      "A merger approved\n",
      "possible merger but\n",
      "off merger discussions\n",
      "its merger proposal\n",
      "a merger offer\n",
      "the merger terms\n",
      "the merger \n",
      "the merger is\n",
      "former merger specialist\n",
      "and merger talks\n",
      "its merger debt\n",
      "original merger plans\n",
      "anticipated merger of\n",
      "the merger it\n",
      "projected merger date\n",
      "the merger transaction\n",
      "revised merger offers\n",
      "whole merger mania\n",
      "the merger in\n"
     ]
    }
   ],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    \n",
    "    ### COMPLETE THE CODE HERE\n",
    "    matches = re.findall(r'[a-zA-Z]* merger [a-zA-Z]*', article['text'])\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look more useful, but we still want to retrieve whole sentences. \n",
    "\n",
    "Sentences in English are usually demarcated by punctuation, so let's use the following punctuation marks to identify sentence boundaries: '.', '!', '?'. Those punctuation marks are special characters when used in regular expressions, so to force Python to interpret them literally, we need to put the escape character '\\' in front of them. \n",
    "\n",
    "Now, we can write a disjunction that matches against the punctuation like this: `[\\.\\!\\?]`.\n",
    "\n",
    "To retrieve a whole sentence, we would like to match all of the text between two punctuation marks. Besides letters, the text of a sentence contains space characters and new line characters, which appear as '\\n'. These are formatting characters that specify a line break. To retrieve a whole sentence, we will need to match against letters, spaces and new line characters.\n",
    "\n",
    "TODO 2: Retrieve all strings containing 'merger', starting from the preceding punctuation mark, until the following punctuation mark. To do this, modify the disjunction that matches against letters to also match spaces and new line characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    \n",
    "    ### COMPLETE THE CODE HERE\n",
    "    matches = re.findall(r'([A-Z]* merger [A-Z]*[\\.\\!\\?])', article['text'])\n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have assumed the sentences consist only of letters, spaces and new lines. Can you think of any characters we have excluded here?\n",
    "\n",
    "A better way to find all matches would be to use _negation_ to match against any character _except_ the punctuation marks that bound the sentences. A negation will match any character except those specified, which we can write like this: `[^\\.\\!\\?]`, where the '^' indicates the negation.\n",
    "\n",
    "TODO 3: Modify the expression you used in the previous cell to use the negation `[^\\.\\!\\?]` to try to find all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    \n",
    "    ### COMPLETE THE CODE HERE\n",
    "    matches = re.findall(, article['text'])\n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Did your improvement return more matching sentences? \n",
    "\n",
    "Are there any problems in our sentence segmentation? \n",
    "\n",
    "There are lots more special characters that you can use to form really powerful regular expressions. If you're interested, you can find a complete list [here](https://docs.python.org/3/library/re.html#regular-expression-syntax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Substitution\n",
    "\n",
    "We can also use regular expressions during _preprocessing_ to clean up text and prepare it for further analysis. For this, we use regular expression _substitution_, which finds a matching string within a larger piece of text, and replaces it with another string.\n",
    "\n",
    "Let's use this to clean up the text by removing the line break characters.\n",
    "\n",
    "In Python, we can use the re.sub() function, which takes three arguments:\n",
    "1. The expression to match. \n",
    "2. The pattern we should replace it with\n",
    "3. The text to apply the subtitution to. \n",
    "\n",
    "To remove the line breaks, the expression in argument 1 can be set to match any non-new line characters with the disjunction `[^\\n]*`. \n",
    "\n",
    "Here, we will also put the disjunctions inside parentheses, like this: `([^\\n]*)`. This specifies that the matching characters are a _group_. This allows the second argument (the replacement pattern) to refer to each _group_ of characters matched by the first argument. \n",
    "\n",
    "Look at the code below to see how this is done, then run it to get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ORIGINAL TEXT: ')\n",
    "print(train_dataset[3]['text'])\n",
    "    \n",
    "clean_article = re.sub(r'([^\\n]*)\\n([^\\n]*)', r'\\1 \\2', train_dataset[3]['text'])\n",
    "    \n",
    "print('CLEAN TEXT: ')\n",
    "print(clean_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Normalisation \n",
    "\n",
    "For most text analytics tasks, we will first need to transform the raw text to a suitable format for input to method such as a classifier. This process is called _text normalisation_ and is part of the _preprocessing_ stage. There are three common steps:\n",
    "\n",
    "1. Sentence segmentation, which we have already been doing with regular expressions. \n",
    "2. Tokenisation, in which the sentences are split into a sequence of tokens, which include words, numbers and punctuation marks.\n",
    "3. Word normalisation, in which different forms of a word are replaced by a root form.\n",
    "\n",
    "We are now going to see how to perform these steps using the NLTK library.\n",
    "\n",
    "## 3.1 Sentence Segmentation\n",
    "\n",
    "Let's start by using NLTK to split a document into sentences. This should give better results than our regular expressions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "article = train_dataset[3]['text']\n",
    "\n",
    "sents = nltk.sent_tokenize(article)\n",
    "\n",
    "for sent in sents[:5]:\n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the first five sentences of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4: Use the regular expression substitution code from section 2.2 to remove the new line characters from the sentences displayed above and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = []\n",
    "\n",
    "for sent in sents[:5]:\n",
    "    \n",
    "    ### COMPLETE YOUR CODE HERE\n",
    "    sent = \n",
    "    #######\n",
    "    \n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the first five sentences of this document\n",
    "    \n",
    "    clean_sents.append(sent)  # save the cleaned sentences for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenisation\n",
    "\n",
    "NLTK provides a similar function for tokenizing the text at the word level. You can find the documentation [here](https://www.nltk.org/api/nltk.tokenize.html). \n",
    "\n",
    "TODO 5: Use word_tokenize() to tokenize each of the sentences from the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = []\n",
    "\n",
    "for sent in clean_sents:\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tokens = \n",
    "    #######\n",
    "    \n",
    "    print(\"<TOKENS>\")\n",
    "    print(tokens)\n",
    "    \n",
    "    tokenized_sents.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how NLTK has handled the non-letter characters. \n",
    "* What does it do with most punctuation marks? \n",
    "* When does it not split tokens based on punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tokenized_sents:\n",
    "    for tok in sent:\n",
    "        if re.search(r'[^a-zA-Z0-9]', tok):  # find the non-letter and non-digit characters\n",
    "            print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Word Normalisation\n",
    "\n",
    "Many words can appear in different forms, including: \n",
    "* Conjugated verbs\n",
    "* Plural and singular nouns\n",
    "* Common abbrevations and synonyms like \"USA\" and \"US\". \n",
    "\n",
    "Mapping all of these surface forms to a single root form reduces the size of the vocabulary that we have to deal with and can therefore improve the performance of text classifiers or topic models.\n",
    "\n",
    "The two most widely used tools for this task in English are the Porter Stemmer and WordNet Lemmatizer. These tools apply a series of regular expression substitutions to tokenised text to convert words to a standard format. \n",
    "* The Porter stemmer is much faster but just removes word prefixes and endings, which leads to some errors. It is often used when real-time or high-volume text processing is needed.\n",
    "* As well as applying regular expressions, lemmatizers look words up in a dictionary to find their root forms, so are more accurate but much slower. \n",
    "\n",
    "Let's start by applying the [Porter Stemmer class](https://www.nltk.org/_modules/nltk/stem/porter.html) to our tokenised text by calling the stem() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer() \n",
    "stemmed_sents = []\n",
    "\n",
    "for sent in tokenized_sents:\n",
    "    stemmed_sent = [stemmer.stem(tok) for tok in sent]\n",
    "    \n",
    "    stemmed_sents.append(stemmed_sent)\n",
    "    \n",
    "    print(\"<STEMMED TOKENS>\")\n",
    "    print(stemmed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the stemming results to lemmatisation. For this task, NLTK provides the [class WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html) with the method lemmatize(). This method takes an argument, `pos`, that determines whether the lemmatizer is applied to nouns, verbs, adjectives or adverbs.\n",
    "\n",
    "TODO 6: Use the WordNetLemmatizer to lemmatize the nouns in the tokenized sentences. Set the `pos` argument to 'n'. \n",
    "\n",
    "TODO 7: Add a second call to lemmatize() to lemmatize the verbs in the sentences as well. Set the `pos` argument to 'v'. \n",
    "\n",
    "How do the results compare with the Porter stemmer? \n",
    "\n",
    "How have the verbs in the sentences changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer() \n",
    "lemma_sents = []\n",
    "for sent in tokenized_sents:\n",
    "    \n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    lemma_sent = \n",
    "    #######\n",
    "    \n",
    "    lemma_sents.append(lemma_sent)\n",
    "    \n",
    "    print(\"<LEMMATIZED TOKENS>\")\n",
    "    print(lemma_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
