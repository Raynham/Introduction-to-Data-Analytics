{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Text Classification\n",
    "\n",
    "This lab explores a new dataset for text classification tasks using naïve Bayes and logistic regression.\n",
    "\n",
    "### Learning Outcomes\n",
    "* Be able to train and test naïve Bayes and logistic regression classifiers using scikit-learn.\n",
    "* Know how to apply evaluation metrics to the classifiers and display examples of misclassifications.\n",
    "* Be able to examine learned model parameters and explain how each classifier makes a decision.\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Load a new Twitter dataset, which is described in [this paper](https://arxiv.org/pdf/2010.12421.pdf), then extracts feature vectors from each sample.\n",
    "1. Training and evaluating naïve Bayes using Scikit-learn.\n",
    "1. Training and evaluating logistic regression using Scikit-learn.\n",
    "1. Optional extension: lemmatization and bigram features.\n",
    "1. Optional extensions: lexicon features.\n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Look out for 'QUESTIONS' which you should try to answer before moving on to the next cell. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to Blackboard (anonymously) or on Teams in the QA channel (with your name), or ask a question in the Wednesday live sessions. \n",
    "\n",
    "As you work through the notebooks, please make a note of any code that is unclear to you.\n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! To understand what's going on inside the methods we use here, make sure to watch the lecture videos for the same week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data \n",
    "\n",
    "This time we are using part of the Tweet Eval dataset, which contains seven Twitter datasets for various social media classification tasks. Here, we'll focus on the sentiment analysis data. \n",
    "Run the code below to download the data from [HuggingFace's datasets hub](https://huggingface.co/datasets/tweet_eval):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 9.72kB [00:00, 2.44MB/s]                   \n",
      "Downloading: 30.4kB [00:00, 4.61MB/s]                   \n",
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 45615 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"train\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 12284 instances loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the test set:\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"test\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the instances in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"', 'label': 2} {'text': \"@user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\", 'label': 1} 2 1\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0], test_dataset[0], train_labels[0],test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenise the text of each tweet and convert it to a bag of words, ready for input to a classifier. \n",
    "To do this, we will use the scikit-learn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data into lists ready for the next steps...\n",
    "train_tweets = [sample['text'] for sample in train_dataset]\n",
    "train_labels = [sample['label'] for sample in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = [sample['text'] for sample in test_dataset]\n",
    "test_labels = [sample['label'] for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a bag of words, we can use the CountVectorizer class ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
    "This class outputs the bag of words as a feature vector, where the length of the vector is equal to the size of the vocabulary, and the values are the counts of each words in a document. \n",
    "\n",
    "Run the code below to obtain feature vectors for the training and test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. We write a small wrapper class to enable this:\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_tweets)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_tweets)  # extract training set bags of words\n",
    "X_test = vectorizer.transform(test_tweets)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method sets the vectorizer up by extracting a vocabulary from some text data. \n",
    "\n",
    "QUESTION: Why do we fit the CountVectorizer on the training set?\n",
    "\n",
    "The vectorizer stores the vocabulary as a dictionary that maps a token to its index in the feature vector. The code below looks up the indexes of some example words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45912\n",
      "23574\n",
      "42635\n",
      "Vocabulary size = 51915\n"
     ]
    }
   ],
   "source": [
    "import reprlib\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "print(vocabulary['the'])\n",
    "print(vocabulary['horse'])\n",
    "print(vocabulary['smile'])\n",
    "\n",
    "print(f'Vocabulary size = {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes Classifier\n",
    "\n",
    "The code above has obtained the feature vectors and lists of labels. The data is now ready for use\n",
    "with scikit-learn's classifiers.\n",
    "\n",
    "Scikit-learn contains several different variants of naïve Bayes for different kinds of data. For our bag of words data, we need to use the [MultinomialNB class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB).\n",
    "\n",
    "\n",
    "TODO 2.1: Look at the documentation for MultinomalNB and write code to train a NB classifier using `X_train` and `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.naive_bayes import MultinomialNB as mnb\n",
    "\n",
    "mnb_model = mnb()\n",
    "mnb_model.fit(X_train,train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have a trained model, we would like to evaluate its performance on some test data. \n",
    "\n",
    "TODO 2.2: Refer to the documentation again and predict the labels for the test set. Use `X_test` as the inputs to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "test_predicted_labels = mnb_model.predict(X_test)\n",
    "print(test_predicted_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can compute standard metrics for classifier performance using [scikit-learn's metrics libary](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules). A useful function for multi-class classification (when there are more than two classes) is the [classification report function](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report).\n",
    "\n",
    "TODO 2.3: Refer again to the documentation, and compute accuracy, precision, recall and F1 scores on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1698 1821  453]\n",
      " [ 793 4015 1129]\n",
      " [  91  758 1526]]\n",
      "Accuracy: 0.589303158580267\n",
      "precision_macro: 0.5858358674079466\n",
      "recall_macro: 0.5820954126917286\n",
      "F1_macro: 0.5718657407887534\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "print(confusion_matrix(test_labels,test_predicted_labels))\n",
    "print(f\"Accuracy: {accuracy_score(test_labels,test_predicted_labels)}\")\n",
    "print(f\"precision_macro: {precision_score(test_labels,test_predicted_labels, average= 'macro')}\")\n",
    "print(f\"recall_macro: {recall_score(test_labels,test_predicted_labels, average= 'macro')}\")\n",
    "print(f\"F1_macro: {f1_score(test_labels,test_predicted_labels, average= 'macro')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's examine the classifier that we learned. If you don't follow what's happening here, you may wish to refer back to the slides on naïve Bayes classifiers or to [Jurafsky and Martin's textbook](https://web.stanford.edu/~jurafsky/slp3/4.pdf). \n",
    "\n",
    "Previously, we trained a MultinomialNB classifier. The trained classifier object stores all the probabilities that it learned during training, which are needed to make predictions. The log of the likelihoods of each word given the class are represented by the attribute `feature_log_prob_`. So, if your classifier object is named `classifier`, you can access the likelihoods with `classifier.feature_log_prob_`.\n",
    "\n",
    "TODO 2.4: Print out the likelihood of the words 'happy' and 'hate' in each class. Hint: look up the index of the chosen words in `vocabulary`. The rows of `feature_log_prob` correspond to classes, and the columns to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy in class 0: 0.00016528408760950073\n",
      "happy in class 1: 0.00010482220248600166\n",
      "happy in class 2: 0.0017517469461780536\n",
      "hate in class 0: 0.000509253675337381\n",
      "hate in class 1: 8.957533666985593e-05\n",
      "hate in class 2: 5.334186803221848e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(mnb_model.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "# WRITE YOUR CODE HERE\n",
    "index_happ = vocabulary[\"happy\"]\n",
    "index_hate = vocabulary[\"hate\"]\n",
    "print(f'happy in class 0: {feat_likelihoods[0][index_happ]}')\n",
    "print(f'happy in class 1: {feat_likelihoods[1][index_happ]}')\n",
    "print(f'happy in class 2: {feat_likelihoods[2][index_happ]}')\n",
    "print(f'hate in class 0: {feat_likelihoods[0][index_hate]}')\n",
    "print(f'hate in class 1: {feat_likelihoods[1][index_hate]}')\n",
    "print(f'hate in class 2: {feat_likelihoods[2][index_hate]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentiment classes are negative (0), neutral (1) and positive (2). \n",
    "\n",
    "QUESTION: Which class has the strongest association with 'happy' and with 'hate'?\n",
    "\n",
    "A key part of evaluating a classifier is investigating the errors it makes to better understand its limitations. \n",
    "\n",
    "TODO 2.5: Complete the code below to print out some misclassified tweets along with their predicted and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: @user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.; true label = 1, prediction = 0.\n",
      "Tweet: @user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.; true label = 0, prediction = 1.\n",
      "Tweet: Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS; true label = 2, prediction = 1.\n",
      "Tweet: @user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.; true label = 0, prediction = 1.\n",
      "Tweet: When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!; true label = 0, prediction = 1.\n",
      "Tweet: @user ohhh ok i see 🤔 what if u have medical marijuana clearance? Does that make a difference; true label = 1, prediction = 0.\n",
      "Tweet: @user alt-right was adopted by Deplorables. Average middle Americans.  I've now moved to Libertarian. @user; true label = 1, prediction = 2.\n",
      "Tweet: @user #GilmoreGirlsTop4 Lorelai, Rory, Lane, Sookie @user @user; true label = 1, prediction = 2.\n",
      "Tweet: Zac Efron Flaunts Fit Abs in New ‘Dirty Grandpa’... #ZacEfron; true label = 1, prediction = 2.\n",
      "Tweet: 1am and I'm still watching #ThisIsUs; true label = 1, prediction = 2.\n"
     ]
    }
   ],
   "source": [
    "error_indexes = test_predicted_labels != test_labels  # compare predictions to gold labels\n",
    "\n",
    "# get the text of tweets where the classifier made an error:\n",
    "tweets_err = np.array(test_tweets)[error_indexes]\n",
    "\n",
    "### WRITE YOUR CODE HERE\n",
    "gold_err = np.array(test_labels)[error_indexes]\n",
    "pred_err = np.array(test_predicted_labels)[error_indexes]\n",
    "\n",
    "for i in range(10):  # just print the first ten\n",
    "    print(f'Tweet: {tweets_err[i]}; true label = {gold_err[i]}, prediction = {pred_err[i]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Logistic Regression Classifier\n",
    "\n",
    "Another simple, linear classifier is logistic regression. This classifier does not rely on the conditional independence assumption, so can better model features that are highly correlated with each other. Scikit-learn provides the [logisticRegression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), which has a very similar interface to the naïve Bayes classifier.\n",
    "\n",
    "TODO 3.1: Train a logistic regression classifier, referring to the scikit-learn documentation as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "lr_model = lr(max_iter=1000)\n",
    "lr_model.fit(X_train,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.2: Obtain predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "lr_predicted_test_labels = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.3: Compute accuracy, precision, recall and F1 scores on the test set using [scikit-learn's metrics libary.](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1684 1895  393]\n",
      " [ 773 4173  991]\n",
      " [ 111  883 1381]]\n",
      "Accuracy: 0.5892217518723543\n",
      "precision_macro: 0.5851886728191571\n",
      "recall_macro: 0.5694405670594046\n",
      "F1_macro: 0.5666393128241448\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "print(confusion_matrix(test_labels,lr_predicted_test_labels))\n",
    "print(f\"Accuracy: {accuracy_score(test_labels,lr_predicted_test_labels)}\")\n",
    "print(f\"precision_macro: {precision_score(test_labels,lr_predicted_test_labels, average= 'macro')}\")\n",
    "print(f\"recall_macro: {recall_score(test_labels,lr_predicted_test_labels, average= 'macro')}\")\n",
    "print(f\"F1_macro: {f1_score(test_labels,lr_predicted_test_labels, average= 'macro')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "QUESTION: How does the performance of logistic regression compare with naïve Bayes?\n",
    "\n",
    "The logistic regression classifier works by learning a weight for each feature that indicates its importance in predicting a class. These weights are stored in the `coef_` attribute of the LogisticRegression object, which has rows corresponding to classes, and columns corresponding to words in the vocabulary. \n",
    "\n",
    "TODO 3.4: Print out the weights for 'happy' and 'hate' for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy in class 0: -0.5334493820618522\n",
      "happy in class 1: -1.278182470820909\n",
      "happy in class 2: 1.8116318528826572\n",
      "hate in class 0: 1.752942170100212\n",
      "hate in class 1: -0.1707323149937449\n",
      "hate in class 2: -1.5822098551065442\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "weights = lr_model.coef_\n",
    "index_happ = vocabulary[\"happy\"]\n",
    "index_hate = vocabulary[\"hate\"]\n",
    "print(f'happy in class 0: {weights[0][index_happ]}')\n",
    "print(f'happy in class 1: {weights[1][index_happ]}')\n",
    "print(f'happy in class 2: {weights[2][index_happ]}')\n",
    "print(f'hate in class 0: {weights[0][index_hate]}')\n",
    "print(f'hate in class 1: {weights[1][index_hate]}')\n",
    "print(f'hate in class 2: {weights[2][index_hate]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Are the weights what you would expect to see?\n",
    "\n",
    "The code below prints out the words with the highest weights for each class. We use numpy's `argsort` function to get the indexes of the sorted weights. Run the code below to show the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! &\n",
      "\n",
      "Weights for class 0:\n",
      "\n",
      ", gim with weight 1.8718823670337783\n",
      "- swag with weight 1.8812327297748193\n",
      "'s vineyard with weight 1.88284119307575\n",
      ", gifford with weight 1.8889765812649049\n",
      "# royalrumble\\u002c with weight 1.9000971755778961\n",
      ": samsung with weight 1.9182479921652589\n",
      "4:30 pm with weight 1.9239383830426249\n",
      ": amazon with weight 2.215260477764178\n",
      ": beijing with weight 2.321718148445376\n",
      "`` eth with weight 2.696849250726388\n",
      "\n",
      "Weights for class 1:\n",
      "\n",
      ". grilled with weight 1.092316270590288\n",
      "# acwsgothenburg with weight 1.111826289823054\n",
      "? awesome with weight 1.11487877888903\n",
      ". sturbridge with weight 1.1293007747438673\n",
      "... omfg with weight 1.1732489838125382\n",
      "# tgif with weight 1.2193472273265198\n",
      "# developer with weight 1.321940615096413\n",
      "123 jefferson with weight 1.357136595856372\n",
      "2011 . with weight 1.4175509940864437\n",
      "20\\u002c but with weight 1.6881558688607985\n",
      "\n",
      "Weights for class 2:\n",
      "\n",
      ". aj with weight 1.7783839168980173\n",
      "'re going with weight 1.8012577376263812\n",
      ", wayne with weight 1.8116318528826572\n",
      "2nd dad with weight 1.817677799072703\n",
      "2015 forbes with weight 1.8305649874496175\n",
      "'re getting with weight 1.9265244424591617\n",
      "# soros with weight 1.9704012571533887\n",
      "* blank with weight 2.0602358647040893\n",
      "* are with weight 2.0625298704707635\n",
      "# nawazsharif with weight 2.262362985799697\n"
     ]
    }
   ],
   "source": [
    "n_feats_to_show = 10\n",
    "\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "for c, weights_c in enumerate(lr_model.coef_):\n",
    "    print(f'\\nWeights for class {c}:\\n')\n",
    "    strongest_idxs = np.argsort(weights_c)[-n_feats_to_show:]\n",
    "    for idx in strongest_idxs:\n",
    "        print(f'{vocab_inverted[idx]} with weight {weights_c[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.5: Use the same code as for naïve Bayes to print out examples of misclassified tweets and their labels. Hint: you should be able to compy and paste your code from above :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: @user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.; true label = 1, prediction = 0.\n",
      "Tweet: I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user; true label = 2, prediction = 1.\n",
      "Tweet: @user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.; true label = 0, prediction = 1.\n",
      "Tweet: Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS; true label = 2, prediction = 1.\n",
      "Tweet: An interesting security vulnerability - albeit not for the everyday car thief; true label = 1, prediction = 2.\n",
      "Tweet: When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!; true label = 0, prediction = 1.\n",
      "Tweet: Swampbitch Nasty Pelosi  loves yelling 'Fire' in the crowded swamp. #blackfriday @user; true label = 0, prediction = 1.\n",
      "Tweet: @user ohhh ok i see 🤔 what if u have medical marijuana clearance? Does that make a difference; true label = 1, prediction = 0.\n",
      "Tweet: @user alt-right was adopted by Deplorables. Average middle Americans.  I've now moved to Libertarian. @user; true label = 1, prediction = 2.\n",
      "Tweet: Zac Efron Flaunts Fit Abs in New ‘Dirty Grandpa’... #ZacEfron; true label = 1, prediction = 2.\n"
     ]
    }
   ],
   "source": [
    "error_indexes = lr_predicted_test_labels != test_labels  # compare predictions to gold labels\n",
    "\n",
    "# get the text of tweets where the classifier made an error:\n",
    "tweets_err = np.array(test_tweets)[error_indexes]\n",
    "\n",
    "### WRITE YOUR CODE HERE\n",
    "gold_err = np.array(test_labels)[error_indexes]\n",
    "pred_err = np.array(lr_predicted_test_labels)[error_indexes]\n",
    "\n",
    "for i in range(10):  # just print the first ten\n",
    "    print(f'Tweet: {tweets_err[i]}; true label = {gold_err[i]}, prediction = {pred_err[i]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Optional: Lemmatization and N-grams\n",
    "\n",
    "You only need to do this section if you finish the previous sections before the end of the lab.\n",
    "\n",
    "In the previous lab, we tried out lemmatization. This is useful for reducing the size of the vocabulary. Could it help us here?\n",
    "\n",
    "To apply lemmatization, we have to go back to the CountVectorizer and define a new tokenizer that will carry out the extra step of lemmatization. Run the code below to test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'qt', '@', 'user', 'in', 'the', 'original', 'draft', 'of', '7th', 'book', ',', 'remus', 'lupin', 'survive', 'battle', 'hogwarts', '.', '#', 'happybirthdayremuslupin']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, tweets):\n",
    "        return [self.wnl.lemmatize(self.wnl.lemmatize(self.wnl.lemmatize(tok, pos='n'), pos='v'), pos='a') for tok in word_tokenize(tweets)]\n",
    "    \n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45324\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.1: Now, repeat your training of the logistic regression using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1807 1730  435]\n",
      " [ 819 3998 1120]\n",
      " [  97  773 1505]]\n",
      "Accuracy: 0.595083034842071\n",
      "precision_macro: 0.5901395641119581\n",
      "recall_macro: 0.5873409428171968\n",
      "F1_macro: 0.5788307669752252\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "mnb_model = mnb()\n",
    "mnb_model.fit(X_train,train_labels)\n",
    "test_predicted_labels = mnb_model.predict(X_test)\n",
    "print(confusion_matrix(test_labels,test_predicted_labels))\n",
    "print(f\"Accuracy: {accuracy_score(test_labels,test_predicted_labels)}\")\n",
    "print(f\"precision_macro: {precision_score(test_labels,test_predicted_labels, average= 'macro')}\")\n",
    "print(f\"recall_macro: {recall_score(test_labels,test_predicted_labels, average= 'macro')}\")\n",
    "print(f\"F1_macro: {f1_score(test_labels,test_predicted_labels, average= 'macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Did lemmatization bring about any improvements on this dataset?\n",
    "\n",
    "The bag of words is a very simple representation of the tweets that does not capture enough information to make accurate sentiment classifications. Another way to improve it could be to use bigrams instead of single words as our features. Bigrams are pairs of words that occur one after another in the text. Bigrams are a kind of 'n-gram', where 'n=2'. \n",
    "\n",
    "To extract bigrams, we again modify our CountVectorizer. This class has a parameter `ngram_range`, which determines the range of sizes of n-grams the vectorizer will include. If we set `ngram_range=(1,1)` we have our standard bag of words. If we set it to `ngram_range=(2,2)`, we use bigrams instead. Choosing If we set `ngram_range=(1,2)` will use both single tokens (unigrams) and bigrams.\n",
    "\n",
    "TODO 4.2: Create a new CountVectorizer that extracts bigram features instead of unigrams (single tokens) and uses the LemmaTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['`` qt', 'qt @', '@ user', 'user in', 'in the', 'the original', 'original draft', 'draft of', 'of the', 'the 7th', '7th book', 'book ,', ', remus', 'remus lupin', 'lupin survived', 'survived the', 'the battle', 'battle of', 'of hogwarts', 'hogwarts .']\n",
      "Vocabulary size: 367568\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer(), ngram_range=(2,2))\n",
    "###\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.3: Now, repeat your training of the logistic regression or naïve Bayes classifier using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 436 2915  621]\n",
      " [ 122 4575 1240]\n",
      " [  14  757 1604]]\n",
      "Accuracy: 0.5385053728427223\n",
      "precision_macro: 0.5932999353152136\n",
      "recall_macro: 0.5185760024612774\n",
      "F1_macro: 0.4621031798498693\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "mnb_model = mnb()\n",
    "mnb_model.fit(X_train,train_labels)\n",
    "test_predicted_labels = mnb_model.predict(X_test)\n",
    "print(confusion_matrix(test_labels,test_predicted_labels))\n",
    "print(f\"Accuracy: {accuracy_score(test_labels,test_predicted_labels)}\")\n",
    "print(f\"precision_macro: {precision_score(test_labels,test_predicted_labels, average= 'macro')}\")\n",
    "print(f\"recall_macro: {recall_score(test_labels,test_predicted_labels, average= 'macro')}\")\n",
    "print(f\"F1_macro: {f1_score(test_labels,test_predicted_labels, average= 'macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "QUESTION: Do bigrams improve performance on this dataset?\n",
    "\n",
    "# 5. Optional: Lexicon Features\n",
    "\n",
    "You only need to do this part if you finish the other parts before the end of the lab session. \n",
    "\n",
    "The NLTK library contains sentiment lexicons, which are lists of words with negative or positive connotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the sentiment scores for some words in the lexicon by running the code below. What do the scores mean and why do some words have no score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testwords = ['happy', 'wonderful', 'horrible', 'boring', 'tablecloth', 'not']\n",
    "\n",
    "for word in testwords:\n",
    "    if word in analyser.lexicon:\n",
    "        print(f'{word}: {analyser.lexicon[word]}')\n",
    "    else:\n",
    "        print(f'{word}: NOT IN LEXICON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to use this function to compute counts of all positive and negative words. Let's start by recording whether the words in our vocabulary are positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Vader lexicon scores for each word in our vocabulary\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "lex_pos_scores = np.zeros((1, len(vocabulary)))\n",
    "lex_neg_scores = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "for i, term in enumerate(vocabulary):\n",
    "    if term in analyser.lexicon and analyser.lexicon[term] > 0:\n",
    "        lex_pos_scores[0, i] = 1\n",
    "    elif term in analyser.lexicon and analyser.lexicon[term] < 0:\n",
    "        lex_neg_scores[0, i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the counts of positive and negative words in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores for each instance in the data set. \n",
    "\n",
    "# Multiply the lexicon scores by the feature vectors, then sum over the \n",
    "# vocabulary to get the total positive and total negative counts:\n",
    "lex_pos_train = np.sum(X_train.multiply(lex_pos_scores), axis=1)\n",
    "lex_pos_test = np.sum(X_test.multiply(lex_pos_scores), axis=1)\n",
    "\n",
    "lex_neg_train = np.sum(X_train.multiply(lex_neg_scores), axis=1)\n",
    "lex_neg_test = np.sum(X_test.multiply(lex_neg_scores), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can append the counts to the feature vector and treat them as extra features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train = hstack((X_train, lex_pos_train, lex_neg_train))\n",
    "X_test = hstack((X_test, lex_pos_test, lex_neg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 5.1: Use the new X_train and X_test feature vectors to train and evaluate your classifier. \n",
    "Does adding the lexicon features improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db6dbd6af82c8a65067133132fa7cc8afd2d273c6e54b5cae29f2d55acec71c4"
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
