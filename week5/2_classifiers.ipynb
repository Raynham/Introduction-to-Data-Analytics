{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Text Classification\n",
    "\n",
    "This lab explores a new dataset for text classification tasks using naïve Bayes and logistic regression.\n",
    "\n",
    "### Learning Outcomes\n",
    "* Be able to train and test naïve Bayes and logistic regression classifiers using scikit-learn.\n",
    "* Know how to apply evaluation metrics to the classifiers and display examples of misclassifications.\n",
    "* Be able to examine learned model parameters and explain how each classifier makes a decision.\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Load a new Twitter dataset, which is described in [this paper](https://arxiv.org/pdf/2010.12421.pdf), then extracts feature vectors from each sample.\n",
    "1. Training and evaluating naïve Bayes using Scikit-learn.\n",
    "1. Training and evaluating logistic regression using Scikit-learn.\n",
    "1. Optional extension: lemmatization and bigram features.\n",
    "1. Optional extensions: lexicon features.\n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Look out for 'QUESTIONS' which you should try to answer before moving on to the next cell. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to Blackboard (anonymously) or on Teams in the QA channel (with your name), or ask a question in the Wednesday live sessions. \n",
    "\n",
    "As you work through the notebooks, please make a note of any code that is unclear to you.\n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! To understand what's going on inside the methods we use here, make sure to watch the lecture videos for the same week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data \n",
    "\n",
    "This time we are using part of the Tweet Eval dataset, which contains seven Twitter datasets for various social media classification tasks. Here, we'll focus on the sentiment analysis data. \n",
    "Run the code below to download the data from [HuggingFace's datasets hub](https://huggingface.co/datasets/tweet_eval):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache\\tweet_eval\\sentiment\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 45615 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"train\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\UoB\\Introduction-to-Data-Analytics\\week5\\2_classifiers.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39m# Load the test set:\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=1'>2</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m load_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtweet_eval\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=3'>4</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msentiment\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=4'>5</a>\u001b[0m     split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=5'>6</a>\u001b[0m     \u001b[39m#ignore_verifications=True,\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=6'>7</a>\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=7'>8</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/UoB/Introduction-to-Data-Analytics/week5/2_classifiers.ipynb#ch0000003?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest dataset with \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_dataset)\u001b[39m}\u001b[39;00m\u001b[39m instances loaded\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:1675\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1671'>1672</a>\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1673'>1674</a>\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1674'>1675</a>\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1675'>1676</a>\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1676'>1677</a>\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1677'>1678</a>\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1678'>1679</a>\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1679'>1680</a>\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1680'>1681</a>\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1681'>1682</a>\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1682'>1683</a>\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1683'>1684</a>\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1684'>1685</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1685'>1686</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1686'>1687</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1688'>1689</a>\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1689'>1690</a>\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:1512\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, script_version, **config_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1509'>1510</a>\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1510'>1511</a>\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1511'>1512</a>\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1512'>1513</a>\u001b[0m     path, revision\u001b[39m=\u001b[39;49mrevision, download_config\u001b[39m=\u001b[39;49mdownload_config, download_mode\u001b[39m=\u001b[39;49mdownload_mode, data_files\u001b[39m=\u001b[39;49mdata_files\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1513'>1514</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1515'>1516</a>\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1516'>1517</a>\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:1147\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1138'>1139</a>\u001b[0m _raise_if_offline_mode_is_enabled()\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1139'>1140</a>\u001b[0m \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# canonical datasets/metrics: github path\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1140'>1141</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m CanonicalDatasetModuleFactory(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1141'>1142</a>\u001b[0m         path,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1142'>1143</a>\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1143'>1144</a>\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1144'>1145</a>\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1145'>1146</a>\u001b[0m         dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1146'>1147</a>\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1147'>1148</a>\u001b[0m \u001b[39melif\u001b[39;00m path\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:  \u001b[39m# users datasets/metrics: s3 path (hub for datasets and s3 for metrics)\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=1148'>1149</a>\u001b[0m     hf_api \u001b[39m=\u001b[39m HfApi(config\u001b[39m.\u001b[39mHF_ENDPOINT)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:522\u001b[0m, in \u001b[0;36mCanonicalDatasetModuleFactory.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=519'>520</a>\u001b[0m revision \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrevision\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=520'>521</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=521'>522</a>\u001b[0m     local_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_loading_script(revision)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=522'>523</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=523'>524</a>\u001b[0m     \u001b[39mif\u001b[39;00m revision \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mHF_SCRIPTS_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:505\u001b[0m, in \u001b[0;36mCanonicalDatasetModuleFactory.download_loading_script\u001b[1;34m(self, revision)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=502'>503</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_loading_script\u001b[39m(\u001b[39mself\u001b[39m, revision: Optional[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=503'>504</a>\u001b[0m     file_path \u001b[39m=\u001b[39m hf_github_url(path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m, revision\u001b[39m=\u001b[39mrevision)\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/load.py?line=504'>505</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cached_path(file_path, download_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:298\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=293'>294</a>\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=295'>296</a>\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=296'>297</a>\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=297'>298</a>\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=298'>299</a>\u001b[0m         url_or_filename,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=299'>300</a>\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=300'>301</a>\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=301'>302</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=302'>303</a>\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=303'>304</a>\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=304'>305</a>\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=305'>306</a>\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=306'>307</a>\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=307'>308</a>\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=308'>309</a>\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=309'>310</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=310'>311</a>\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=311'>312</a>\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=312'>313</a>\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:561\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=558'>559</a>\u001b[0m     connected \u001b[39m=\u001b[39m ftp_head(url)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=559'>560</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=560'>561</a>\u001b[0m     response \u001b[39m=\u001b[39m http_head(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=561'>562</a>\u001b[0m         url,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=562'>563</a>\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=563'>564</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=564'>565</a>\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=565'>566</a>\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=566'>567</a>\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=567'>568</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=568'>569</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:  \u001b[39m# ok\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=569'>570</a>\u001b[0m         etag \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m use_etag \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:476\u001b[0m, in \u001b[0;36mhttp_head\u001b[1;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=473'>474</a>\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=474'>475</a>\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=475'>476</a>\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=476'>477</a>\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=477'>478</a>\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=478'>479</a>\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=479'>480</a>\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=480'>481</a>\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=481'>482</a>\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=482'>483</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=483'>484</a>\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=484'>485</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=485'>486</a>\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:405\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=402'>403</a>\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=403'>404</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=404'>405</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(), url\u001b[39m=\u001b[39murl, timeout\u001b[39m=\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=405'>406</a>\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/datasets/utils/file_utils.py?line=406'>407</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/api.py?line=56'>57</a>\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/api.py?line=57'>58</a>\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/api.py?line=58'>59</a>\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/api.py?line=59'>60</a>\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/api.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=523'>524</a>\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=524'>525</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=525'>526</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=526'>527</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=527'>528</a>\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=528'>529</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=530'>531</a>\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=641'>642</a>\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=643'>644</a>\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=644'>645</a>\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=646'>647</a>\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/sessions.py?line=647'>648</a>\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=437'>438</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=439'>440</a>\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=440'>441</a>\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=441'>442</a>\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=442'>443</a>\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=443'>444</a>\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=444'>445</a>\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=445'>446</a>\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=446'>447</a>\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=447'>448</a>\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=448'>449</a>\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=449'>450</a>\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=450'>451</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=452'>453</a>\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=453'>454</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/requests/adapters.py?line=454'>455</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=699'>700</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=701'>702</a>\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=702'>703</a>\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=703'>704</a>\u001b[0m     conn,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=704'>705</a>\u001b[0m     method,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=705'>706</a>\u001b[0m     url,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=706'>707</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=707'>708</a>\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=708'>709</a>\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=709'>710</a>\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=710'>711</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=712'>713</a>\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=713'>714</a>\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=714'>715</a>\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=715'>716</a>\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=716'>717</a>\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=383'>384</a>\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=384'>385</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=385'>386</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=386'>387</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=387'>388</a>\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=388'>389</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1037'>1038</a>\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1039'>1040</a>\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1041'>1042</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1042'>1043</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1043'>1044</a>\u001b[0m         (\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1044'>1045</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1049'>1050</a>\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connectionpool.py?line=1050'>1051</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:416\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=406'>407</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=407'>408</a>\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=408'>409</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=411'>412</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=412'>413</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=413'>414</a>\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=415'>416</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=416'>417</a>\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=417'>418</a>\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=418'>419</a>\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=419'>420</a>\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=420'>421</a>\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=421'>422</a>\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=422'>423</a>\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=423'>424</a>\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=424'>425</a>\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=425'>426</a>\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=426'>427</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=428'>429</a>\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=429'>430</a>\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=430'>431</a>\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=431'>432</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=432'>433</a>\u001b[0m     default_ssl_context\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=433'>434</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=434'>435</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=435'>436</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/connection.py?line=436'>437</a>\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=436'>437</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=437'>438</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=438'>439</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=444'>445</a>\u001b[0m         SNIMissingWarning,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=445'>446</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=447'>448</a>\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=448'>449</a>\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=449'>450</a>\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=450'>451</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=451'>452</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=452'>453</a>\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=489'>490</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=491'>492</a>\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=492'>493</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=493'>494</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/urllib3/util/ssl_.py?line=494'>495</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:512\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=505'>506</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=506'>507</a>\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=507'>508</a>\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=508'>509</a>\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=509'>510</a>\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=510'>511</a>\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=511'>512</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=512'>513</a>\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=513'>514</a>\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=514'>515</a>\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=515'>516</a>\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=516'>517</a>\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=517'>518</a>\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=518'>519</a>\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=519'>520</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1070\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1066'>1067</a>\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1067'>1068</a>\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1068'>1069</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1069'>1070</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1070'>1071</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1071'>1072</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1341\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1338'>1339</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1339'>1340</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1340'>1341</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1341'>1342</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/ssl.py?line=1342'>1343</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the test set:\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"test\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the instances in the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenise the text of each tweet and convert it to a bag of words, ready for input to a classifier. \n",
    "To do this, we will use the scikit-learn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data into lists ready for the next steps...\n",
    "train_tweets = [sample['text'] for sample in train_dataset]\n",
    "train_labels = [sample['label'] for sample in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"', 'label': 2} {'text': \"@user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\", 'label': 1} 2 1\n"
     ]
    }
   ],
   "source": [
    "test_tweets = [sample['text'] for sample in test_dataset]\n",
    "test_labels = [sample['label'] for sample in test_dataset]\n",
    "print(train_dataset[0], test_dataset[0], train_labels[0],test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a bag of words, we can use the CountVectorizer class ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
    "This class outputs the bag of words as a feature vector, where the length of the vector is equal to the size of the vocabulary, and the values are the counts of each words in a document. \n",
    "\n",
    "Run the code below to obtain feature vectors for the training and test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. We write a small wrapper class to enable this:\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_tweets)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_tweets)  # extract training set bags of words\n",
    "X_test = vectorizer.transform(test_tweets)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method sets the vectorizer up by extracting a vocabulary from some text data. \n",
    "\n",
    "QUESTION: Why do we fit the CountVectorizer on the training set?\n",
    "\n",
    "The vectorizer stores the vocabulary as a dictionary that maps a token to its index in the feature vector. The code below looks up the indexes of some example words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45912\n",
      "23574\n",
      "42635\n",
      "Vocabulary size = 51915\n"
     ]
    }
   ],
   "source": [
    "import reprlib\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "print(vocabulary['the'])\n",
    "print(vocabulary['horse'])\n",
    "print(vocabulary['smile'])\n",
    "\n",
    "print(f'Vocabulary size = {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes Classifier\n",
    "\n",
    "The code above has obtained the feature vectors and lists of labels. The data is now ready for use\n",
    "with scikit-learn's classifiers.\n",
    "\n",
    "Scikit-learn contains several different variants of naïve Bayes for different kinds of data. For our bag of words data, we need to use the [MultinomialNB class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB).\n",
    "\n",
    "\n",
    "TODO 2.1: Look at the documentation for MultinomalNB and write code to train a NB classifier using `X_train` and `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.naive_bayes import MultinomialNB as mnb\n",
    "\n",
    "mnb_model = mnb()\n",
    "mnb_model.fit(X_train,train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have a trained model, we would like to evaluate its performance on some test data. \n",
    "\n",
    "TODO 2.2: Refer to the documentation again and predict the labels for the test set. Use `X_test` as the inputs to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "test_predicted_labels = mnb_model.predict(X_test)\n",
    "print(test_predicted_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can compute standard metrics for classifier performance using [scikit-learn's metrics libary](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules). A useful function for multi-class classification (when there are more than two classes) is the [classification report function](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report).\n",
    "\n",
    "TODO 2.3: Refer again to the documentation, and compute accuracy, precision, recall and F1 scores on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.43      0.66      0.52      2582\n",
      "     class 1       0.68      0.61      0.64      6594\n",
      "     class 2       0.64      0.49      0.56      3108\n",
      "\n",
      "    accuracy                           0.59     12284\n",
      "   macro avg       0.58      0.59      0.57     12284\n",
      "weighted avg       0.62      0.59      0.59     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(test_predicted_labels, test_labels, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's examine the classifier that we learned. If you don't follow what's happening here, you may wish to refer back to the slides on naïve Bayes classifiers or to [Jurafsky and Martin's textbook](https://web.stanford.edu/~jurafsky/slp3/4.pdf). \n",
    "\n",
    "Previously, we trained a MultinomialNB classifier. The trained classifier object stores all the probabilities that it learned during training, which are needed to make predictions. The log of the likelihoods of each word given the class are represented by the attribute `feature_log_prob_`. So, if your classifier object is named `classifier`, you can access the likelihoods with `classifier.feature_log_prob_`.\n",
    "\n",
    "TODO 2.4: Print out the likelihood of the words 'happy' and 'hate' in each class. Hint: look up the index of the chosen words in `vocabulary`. The rows of `feature_log_prob` correspond to classes, and the columns to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy in class 0: 0.00016528408760950073\n",
      "happy in class 1: 0.00010482220248600166\n",
      "happy in class 2: 0.0017517469461780536\n",
      "hate in class 0: 0.000509253675337381\n",
      "hate in class 1: 8.957533666985593e-05\n",
      "hate in class 2: 5.334186803221848e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(mnb_model.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "# WRITE YOUR CODE HERE\n",
    "index_happ = vocabulary[\"happy\"]\n",
    "index_hate = vocabulary[\"hate\"]\n",
    "print(f'happy in class 0: {feat_likelihoods[0][index_happ]}')\n",
    "print(f'happy in class 1: {feat_likelihoods[1][index_happ]}')\n",
    "print(f'happy in class 2: {feat_likelihoods[2][index_happ]}')\n",
    "print(f'hate in class 0: {feat_likelihoods[0][index_hate]}')\n",
    "print(f'hate in class 1: {feat_likelihoods[1][index_hate]}')\n",
    "print(f'hate in class 2: {feat_likelihoods[2][index_hate]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentiment classes are negative (0), neutral (1) and positive (2). \n",
    "\n",
    "QUESTION: Which class has the strongest association with 'happy' and with 'hate'?\n",
    "\n",
    "A key part of evaluating a classifier is investigating the errors it makes to better understand its limitations. \n",
    "\n",
    "TODO 2.5: Complete the code below to print out some misclassified tweets along with their predicted and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12284\n",
      "Tweet: @user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.; true label = 1, prediction = 0.\n",
      "Tweet: @user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.; true label = 0, prediction = 1.\n",
      "Tweet: Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS; true label = 2, prediction = 1.\n",
      "Tweet: @user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.; true label = 0, prediction = 1.\n",
      "Tweet: When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!; true label = 0, prediction = 1.\n",
      "Tweet: @user ohhh ok i see 🤔 what if u have medical marijuana clearance? Does that make a difference; true label = 1, prediction = 0.\n",
      "Tweet: @user alt-right was adopted by Deplorables. Average middle Americans.  I've now moved to Libertarian. @user; true label = 1, prediction = 2.\n",
      "Tweet: @user #GilmoreGirlsTop4 Lorelai, Rory, Lane, Sookie @user @user; true label = 1, prediction = 2.\n",
      "Tweet: Zac Efron Flaunts Fit Abs in New ‘Dirty Grandpa’... #ZacEfron; true label = 1, prediction = 2.\n",
      "Tweet: 1am and I'm still watching #ThisIsUs; true label = 1, prediction = 2.\n"
     ]
    }
   ],
   "source": [
    "error_indexes = test_predicted_labels != test_labels  # compare predictions to gold labels\n",
    "print(len(error_indexes))\n",
    "# get the text of tweets where the classifier made an error:\n",
    "tweets_err = np.array(test_tweets)[error_indexes]\n",
    "\n",
    "### WRITE YOUR CODE HERE\n",
    "gold_err = np.array(test_labels)[error_indexes]\n",
    "pred_err = np.array(test_predicted_labels)[error_indexes]\n",
    "\n",
    "for i in range(10):  # just print the first ten\n",
    "    print(f'Tweet: {tweets_err[i]}; true label = {gold_err[i]}, prediction = {pred_err[i]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Logistic Regression Classifier\n",
    "\n",
    "Another simple, linear classifier is logistic regression. This classifier does not rely on the conditional independence assumption, so can better model features that are highly correlated with each other. Scikit-learn provides the [logisticRegression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), which has a very similar interface to the naïve Bayes classifier.\n",
    "\n",
    "TODO 3.1: Train a logistic regression classifier, referring to the scikit-learn documentation as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "lr_model = lr(max_iter=1000)\n",
    "lr_model.fit(X_train,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.2: Obtain predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "lr_predicted_test_labels = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.3: Compute accuracy, precision, recall and F1 scores on the test set using [scikit-learn's metrics libary.](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.42      0.66      0.51      2568\n",
      "     class 1       0.70      0.60      0.65      6951\n",
      "     class 2       0.58      0.50      0.54      2765\n",
      "\n",
      "    accuracy                           0.59     12284\n",
      "   macro avg       0.57      0.59      0.57     12284\n",
      "weighted avg       0.62      0.59      0.59     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(lr_predicted_test_labels, test_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "QUESTION: How does the performance of logistic regression compare with naïve Bayes?\n",
    "\n",
    "The logistic regression classifier works by learning a weight for each feature that indicates its importance in predicting a class. These weights are stored in the `coef_` attribute of the LogisticRegression object, which has rows corresponding to classes, and columns corresponding to words in the vocabulary. \n",
    "\n",
    "TODO 3.4: Print out the weights for 'happy' and 'hate' for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy in class 0: -0.5335792742995011\n",
      "happy in class 1: -1.2779543000406763\n",
      "happy in class 2: 1.8115335743401884\n",
      "hate in class 0: 1.7530872087713203\n",
      "hate in class 1: -0.17058100984881422\n",
      "hate in class 2: -1.582506198922476\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "weights = lr_model.coef_\n",
    "index_happ = vocabulary[\"happy\"]\n",
    "index_hate = vocabulary[\"hate\"]\n",
    "print(f'happy in class 0: {weights[0][index_happ]}')\n",
    "print(f'happy in class 1: {weights[1][index_happ]}')\n",
    "print(f'happy in class 2: {weights[2][index_happ]}')\n",
    "print(f'hate in class 0: {weights[0][index_hate]}')\n",
    "print(f'hate in class 1: {weights[1][index_hate]}')\n",
    "print(f'hate in class 2: {weights[2][index_hate]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Are the weights what you would expect to see?\n",
    "\n",
    "The code below prints out the words with the highest weights for each class. We use numpy's `argsort` function to get the indexes of the sorted weights. Run the code below to show the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights for class 0:\n",
      "\n",
      "fucked with weight 1.8719001739520214\n",
      "horrible with weight 1.8810811722403882\n",
      "disappointing with weight 1.8826870906457467\n",
      "fuck with weight 1.8890126298389327\n",
      "asshole with weight 1.9000627856350742\n",
      "terrible with weight 1.9186021292858315\n",
      "ruined with weight 1.9238251486899896\n",
      "stupid with weight 2.2156346849764352\n",
      "sucks with weight 2.321729398187835\n",
      "worst with weight 2.697173777649992\n",
      "\n",
      "Weights for class 1:\n",
      "\n",
      "jab with weight 1.092118742018886\n",
      "1977 with weight 1.1116859940321\n",
      "uc with weight 1.1146564729729118\n",
      "klub with weight 1.1291464808202143\n",
      "load with weight 1.173361579803787\n",
      "bama with weight 1.2190830130882138\n",
      "50/50 with weight 1.3216677124489193\n",
      "morris with weight 1.3563773417873466\n",
      "paterno with weight 1.4175607934836516\n",
      "phase2 with weight 1.688193658393043\n",
      "\n",
      "Weights for class 2:\n",
      "\n",
      "impressive with weight 1.778428553491804\n",
      "congratulations with weight 1.801213015155426\n",
      "happy with weight 1.8115335743401884\n",
      "proud with weight 1.8179080217177876\n",
      "perfect with weight 1.8308137789738077\n",
      "congrats with weight 1.926288252344832\n",
      "awesome with weight 1.9707379820763655\n",
      "exciting with weight 2.0598392291196714\n",
      "excited with weight 2.06248128498021\n",
      "amazing with weight 2.262813975637703\n"
     ]
    }
   ],
   "source": [
    "n_feats_to_show = 10\n",
    "\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "for c, weights_c in enumerate(lr_model.coef_):\n",
    "    print(f'\\nWeights for class {c}:\\n')\n",
    "    strongest_idxs = np.argsort(weights_c)[-n_feats_to_show:]\n",
    "    for idx in strongest_idxs:\n",
    "        print(f'{vocab_inverted[idx]} with weight {weights_c[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.5: Use the same code as for naïve Bayes to print out examples of misclassified tweets and their labels. Hint: you should be able to compy and paste your code from above :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: @user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.; true label = 1, prediction = 0.\n",
      "Tweet: I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user; true label = 2, prediction = 1.\n",
      "Tweet: @user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.; true label = 0, prediction = 1.\n",
      "Tweet: Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS; true label = 2, prediction = 1.\n",
      "Tweet: An interesting security vulnerability - albeit not for the everyday car thief; true label = 1, prediction = 2.\n",
      "Tweet: When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!; true label = 0, prediction = 1.\n",
      "Tweet: Swampbitch Nasty Pelosi  loves yelling 'Fire' in the crowded swamp. #blackfriday @user; true label = 0, prediction = 1.\n",
      "Tweet: @user ohhh ok i see 🤔 what if u have medical marijuana clearance? Does that make a difference; true label = 1, prediction = 0.\n",
      "Tweet: @user alt-right was adopted by Deplorables. Average middle Americans.  I've now moved to Libertarian. @user; true label = 1, prediction = 2.\n",
      "Tweet: Zac Efron Flaunts Fit Abs in New ‘Dirty Grandpa’... #ZacEfron; true label = 1, prediction = 2.\n"
     ]
    }
   ],
   "source": [
    "error_indexes = lr_predicted_test_labels != test_labels  # compare predictions to gold labels\n",
    "\n",
    "# get the text of tweets where the classifier made an error:\n",
    "tweets_err = np.array(test_tweets)[error_indexes]\n",
    "\n",
    "### WRITE YOUR CODE HERE\n",
    "gold_err = np.array(test_labels)[error_indexes]\n",
    "pred_err = np.array(lr_predicted_test_labels)[error_indexes]\n",
    "\n",
    "for i in range(10):  # just print the first ten\n",
    "    print(f'Tweet: {tweets_err[i]}; true label = {gold_err[i]}, prediction = {pred_err[i]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Optional: Lemmatization and N-grams\n",
    "\n",
    "You only need to do this section if you finish the previous sections before the end of the lab.\n",
    "\n",
    "In the previous lab, we tried out lemmatization. This is useful for reducing the size of the vocabulary. Could it help us here?\n",
    "\n",
    "To apply lemmatization, we have to go back to the CountVectorizer and define a new tokenizer that will carry out the extra step of lemmatization. Run the code below to test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'qt', '@', 'user', 'in', 'the', 'original', 'draft', 'of', '7th', 'book', ',', 'remus', 'lupin', 'survive', 'battle', 'hogwarts', '.', '#', 'happybirthdayremuslupin']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, tweets):\n",
    "        return [self.wnl.lemmatize(self.wnl.lemmatize(self.wnl.lemmatize(tok, pos='n'), pos='v'), pos='a') for tok in word_tokenize(tweets)]\n",
    "    \n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45324\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.1: Now, repeat your training of the logistic regression using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.45      0.66      0.54      2723\n",
      "     class 1       0.67      0.61      0.64      6501\n",
      "     class 2       0.63      0.49      0.55      3060\n",
      "\n",
      "    accuracy                           0.60     12284\n",
      "   macro avg       0.59      0.59      0.58     12284\n",
      "weighted avg       0.62      0.60      0.60     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "mnb_model = mnb()\n",
    "mnb_model.fit(X_train,train_labels)\n",
    "test_predicted_labels = mnb_model.predict(X_test)\n",
    "print(classification_report(test_predicted_labels, test_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Did lemmatization bring about any improvements on this dataset?\n",
    "\n",
    "The bag of words is a very simple representation of the tweets that does not capture enough information to make accurate sentiment classifications. Another way to improve it could be to use bigrams instead of single words as our features. Bigrams are pairs of words that occur one after another in the text. Bigrams are a kind of 'n-gram', where 'n=2'. \n",
    "\n",
    "To extract bigrams, we again modify our CountVectorizer. This class has a parameter `ngram_range`, which determines the range of sizes of n-grams the vectorizer will include. If we set `ngram_range=(1,1)` we have our standard bag of words. If we set it to `ngram_range=(2,2)`, we use bigrams instead. Choosing If we set `ngram_range=(1,2)` will use both single tokens (unigrams) and bigrams.\n",
    "\n",
    "TODO 4.2: Create a new CountVectorizer that extracts bigram features instead of unigrams (single tokens) and uses the LemmaTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'qt', '@', 'user', 'in', 'the', 'original', 'draft', 'of', '7th', 'book', ',', 'remus', 'lupin', 'survived', 'battle', 'hogwarts', '.', '#', 'happybirthdayremuslupin']\n",
      "Vocabulary size: 51915\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer(), ngram_range=(1,1))\n",
    "vectorizer.fit(train_tweets)\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['september has', 'arrived ,', 'means apple', 'from becoming', 'becoming an', 'official thing', 'maguire had', 'had opened', 'in hilton', 'hilton head', 'head till', '8th lol', 'lol go', 'aldean sept.', 'sept. 19th', '! alot', 'president richard', 'trumka as', 'he explores', 'explores whether']\n",
      "Vocabulary size: 419483\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=Tokenizer(), ngram_range=(1,2))\n",
    "vectorizer.fit(train_tweets)\n",
    "print(list(vectorizer.vocabulary_)[-20:])\n",
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "['.']\n",
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "  (0, 1712)\t1\n",
      "  (0, 4368)\t1\n",
      "  (0, 10297)\t1\n",
      "  (0, 19011)\t1\n",
      "  (0, 22105)\t1\n",
      "  (0, 24831)\t1\n",
      "  (0, 24833)\t1\n",
      "  (0, 46525)\t1\n",
      "  (0, 46555)\t1\n",
      "  (0, 53100)\t1\n",
      "  (0, 53734)\t1\n",
      "  (0, 54644)\t1\n",
      "  (0, 56453)\t1\n",
      "  (0, 90676)\t1\n",
      "  (0, 90693)\t1\n",
      "  (0, 102180)\t1\n",
      "  (0, 102188)\t1\n",
      "  (0, 144006)\t1\n",
      "  (0, 144027)\t1\n",
      "  (0, 186451)\t1\n",
      "  (0, 186452)\t1\n",
      "  (0, 195916)\t1\n",
      "  (0, 195917)\t1\n",
      "  (0, 204054)\t1\n",
      "  (0, 206379)\t1\n",
      "  :\t:\n",
      "  (45614, 279352)\t1\n",
      "  (45614, 298576)\t2\n",
      "  (45614, 298625)\t1\n",
      "  (45614, 298644)\t1\n",
      "  (45614, 311485)\t1\n",
      "  (45614, 311486)\t1\n",
      "  (45614, 312074)\t1\n",
      "  (45614, 312089)\t1\n",
      "  (45614, 369580)\t1\n",
      "  (45614, 369817)\t1\n",
      "  (45614, 371781)\t1\n",
      "  (45614, 380633)\t1\n",
      "  (45614, 380634)\t1\n",
      "  (45614, 383703)\t1\n",
      "  (45614, 383742)\t1\n",
      "  (45614, 392713)\t1\n",
      "  (45614, 392717)\t1\n",
      "  (45614, 397987)\t1\n",
      "  (45614, 397991)\t1\n",
      "  (45614, 403687)\t1\n",
      "  (45614, 403698)\t1\n",
      "  (45614, 405626)\t1\n",
      "  (45614, 405949)\t1\n",
      "  (45614, 407489)\t1\n",
      "  (45614, 407681)\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "print(vocab_inverted[24831])\n",
    "print(vectorizer.get_feature_names()[24831:24832])\n",
    "print(train_tweets[0])\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.3: Now, repeat your training of the logistic regression or naïve Bayes classifier using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.11      0.76      0.19       572\n",
      "     class 1       0.77      0.55      0.65      8247\n",
      "     class 2       0.68      0.46      0.55      3465\n",
      "\n",
      "    accuracy                           0.54     12284\n",
      "   macro avg       0.52      0.59      0.46     12284\n",
      "weighted avg       0.71      0.54      0.60     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "mnb_model = mnb()\n",
    "mnb_model.fit(X_train,train_labels)\n",
    "test_predicted_labels = mnb_model.predict(X_test)\n",
    "print(classification_report(test_predicted_labels, test_labels, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "QUESTION: Do bigrams improve performance on this dataset?\n",
    "\n",
    "# 5. Optional: Lexicon Features\n",
    "\n",
    "You only need to do this part if you finish the other parts before the end of the lab session. \n",
    "\n",
    "The NLTK library contains sentiment lexicons, which are lists of words with negative or positive connotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\12055\\AppDat\n",
      "[nltk_data]     a\\Local\\Programs\\Python\\Python310\\lib\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the sentiment scores for some words in the lexicon by running the code below. What do the scores mean and why do some words have no score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy: 2.7\n",
      "wonderful: 2.7\n",
      "horrible: -2.5\n",
      "boring: -1.3\n",
      "tablecloth: NOT IN LEXICON\n",
      "not: NOT IN LEXICON\n"
     ]
    }
   ],
   "source": [
    "testwords = ['happy', 'wonderful', 'horrible', 'boring', 'tablecloth', 'not']\n",
    "\n",
    "for word in testwords:\n",
    "    if word in analyser.lexicon:\n",
    "        print(f'{word}: {analyser.lexicon[word]}')\n",
    "    else:\n",
    "        print(f'{word}: NOT IN LEXICON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to use this function to compute counts of all positive and negative words. Let's start by recording whether the words in our vocabulary are positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'qt', '@', 'user', 'in', 'the', 'original', 'draft', 'of', '7th', 'book', ',', 'remus', 'lupin', 'survived', 'battle', 'hogwarts', '.', '#', 'happybirthdayremuslupin']\n"
     ]
    }
   ],
   "source": [
    "# get the Vader lexicon scores for each word in our vocabulary\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "lex_pos_scores = np.zeros((1, len(vocabulary)))\n",
    "lex_neg_scores = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "for i, term in enumerate(vocabulary):\n",
    "    if term in analyser.lexicon and analyser.lexicon[term] > 0:\n",
    "        lex_pos_scores[0, i] = 1\n",
    "    elif term in analyser.lexicon and analyser.lexicon[term] < 0:\n",
    "        lex_neg_scores[0, i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 51915)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the counts of positive and negative words in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores for each instance in the data set. \n",
    "# Multiply the lexicon scores by the feature vectors, then sum over the \n",
    "# vocabulary to get the total positive and total negative counts:\n",
    "lex_pos_train = np.sum(X_train.multiply(lex_pos_scores), axis=1)\n",
    "lex_pos_test = np.sum(X_test.multiply(lex_pos_scores), axis=1)\n",
    "lex_neg_train = np.sum(X_train.multiply(lex_neg_scores), axis=1)\n",
    "lex_neg_test = np.sum(X_test.multiply(lex_neg_scores), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can append the counts to the feature vector and treat them as extra features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train = hstack((X_train, lex_pos_train, lex_neg_train))\n",
    "X_test = hstack((X_test, lex_pos_test, lex_neg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 51917)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 5.1: Use the new X_train and X_test feature vectors to train and evaluate your classifier. \n",
    "Does adding the lexicon features improve performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.40      0.67      0.50      2385\n",
      "     class 1       0.70      0.60      0.65      6848\n",
      "     class 2       0.64      0.50      0.56      3051\n",
      "\n",
      "    accuracy                           0.59     12284\n",
      "   macro avg       0.58      0.59      0.57     12284\n",
      "weighted avg       0.62      0.59      0.60     12284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "mnb_model = mnb()\n",
    "mnb_model.fit(X_train,train_labels)\n",
    "test_predicted_labels = mnb_model.predict(X_test)\n",
    "print(classification_report(test_predicted_labels, test_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'qt', '@', 'user', 'in', 'the', 'original', 'draft', 'of', '7th', 'book', ',', 'remus', 'lupin', 'survived', 'battle', 'hogwarts', '.', '#', 'happybirthdayremuslupin']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.42      0.66      0.52      2573\n",
      "     class 1       0.70      0.60      0.65      6955\n",
      "     class 2       0.58      0.50      0.54      2756\n",
      "\n",
      "    accuracy                           0.59     12284\n",
      "   macro avg       0.57      0.59      0.57     12284\n",
      "weighted avg       0.62      0.59      0.60     12284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "lex_pos_scores = np.zeros((1, len(vocabulary)))\n",
    "lex_neg_scores = np.zeros((1, len(vocabulary)))\n",
    "for i, term in enumerate(vocabulary):\n",
    "    if term in analyser.lexicon and analyser.lexicon[term] > 0:\n",
    "        lex_pos_scores[0, i] = analyser.lexicon[term]\n",
    "    elif term in analyser.lexicon and analyser.lexicon[term] < 0:\n",
    "        lex_neg_scores[0, i] = analyser.lexicon[term]\n",
    "lex_pos_train = np.sum(X_train.multiply(lex_pos_scores), axis=1)\n",
    "lex_pos_test = np.sum(X_test.multiply(lex_pos_scores), axis=1)\n",
    "lex_neg_train = np.sum(X_train.multiply(lex_neg_scores), axis=1)\n",
    "lex_neg_test = np.sum(X_test.multiply(lex_neg_scores), axis=1)\n",
    "X_train = hstack((X_train, lex_pos_train, lex_neg_train))\n",
    "X_test = hstack((X_test, lex_pos_test, lex_neg_test))\n",
    "lr_model = lr(max_iter=1000)\n",
    "lr_model.fit(X_train,train_labels)\n",
    "test_predicted_labels = lr_model.predict(X_test)\n",
    "print(classification_report(test_predicted_labels, test_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2059.9999999999995"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lex_neg_scores)\n",
    "np.sum(lex_pos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_list = [\"Research shows you could be better off by up to £43k over 30 years of investing in an ii ISA and Trading Account due to our low, flat fees. This is just for illustration if all other factors were the same.\",\n",
    "\"The advantage of lower fees over time means that you could be significantly better off in the long run. And by holding multiple accounts with ii for the same low fee, you can save even more.\", \n",
    "\"Transfer your ISA and Trading Accounts to ii and keep more of your money.\",\n",
    "\"Please remember, investment value can go up or down and you could get back less than you invest. The value of international investments may be affected by currency fluctuations which might reduce their value in sterling.\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['research', 'shows', 'you', 'could', 'be', 'better', 'off', 'by', 'up', 'to', '£43k', 'over', '30', 'years', 'of', 'investing', 'in', 'an', 'ii', 'isa']\n",
      "Vocabulary size: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12055\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer(), ngram_range=(1,1))\n",
    "vectorizer.fit(sen_list)\n",
    "X_train = vectorizer.transform(sen_list)\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['invest .', '. the', 'the value', 'value of', 'of international', 'international investments', 'investments may', 'may be', 'be affected', 'affected by', 'by currency', 'currency fluctuations', 'fluctuations which', 'which might', 'might reduce', 'reduce their', 'their value', 'value in', 'in sterling', 'sterling .']\n",
      "Vocabulary size: 212\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer(), ngram_range=(1,2))\n",
    "vectorizer.fit(sen_list)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "print(list(vectorizer.vocabulary_)[-20:])\n",
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db6dbd6af82c8a65067133132fa7cc8afd2d273c6e54b5cae29f2d55acec71c4"
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
